Tensorflow Lite demos with input and display via OpenCV and TIDL offload for AM5

Procedure to build
----------------------------
Step 1: Set environment variables
    * Arm only:
      - export SYSROOT_INCDIR (if all dependent header files are under the same directoy), or
        export FLATBUFFERS_INC_DIR, TFLITE_INC_DIR, and OPENCV_INC_DIR
      - export SYSROOT_LIBDIR (if all dependent libs are under the same directoy), or
        export TFLITE_LIB_DIR and OPENCV_LIB_DIR
      - export TARGET_TOOLCHAIN_PREFIX=arm-linux-gnueabihf-
      - export PATH=/[path]/gcc-arm-2019.03/bin:$PATH

    * Add the following for TIDL offload to AM5
      - export TIDL_ACC=yes
      - export TIDL_API_DIR
      - export TIDL_API_LIB_DIR (if tidl-api lib is not under SYSROOT_LIBDIR)

Step 2: Run "make" from the top-level directory to build the demos

Example:
  $ export SYSROOT_INCDIR="/oe-layersetup/build/arago-tmp-external-arm-toolchain/work/am57xx_evm-linux-gnueabi/tensorflow-lite-demo/01.00.00-r1/recipe-sysroot/usr/include"
  $ export SYSROOT_LIBDIR="/oe-layersetup/build/arago-tmp-external-arm-toolchain/work/am57xx_evm-linux-gnueabi/tensorflow-lite-demo/01.00.00-r1/recipe-sysroot/usr/lib"
  $ export TARGET_TOOLCHAIN_PREFIX=arm-linux-gnueabihf-
  $ export PATH=/tools/gcc-arm-2019.03/bin:$PATH
  $ export TIDL_API_DIR="/oe-layersetup/build/arago-tmp-external-arm-toolchain/work/am57xx_evm-linux-gnueabi/tensorflow-lite-demo/01.00.00-r1/recipe-sysroot/usr/share/ti/tidl"
  $ export TIDL_ACC=yes

  $ make

Binaries to run on target
---------------------------

* Classification: run "tflite_classification" with command usage as below:
--tflite_model, -m: model_name.tflite
--input_src, -r: [0|1|2] input source: image 0, video 1, camera 2
--input_path, -i: path of the input image/video or video port for camera, e.g., 1 for /dev/video1
--labels, -l: labels for the model
--frame_cnt, -c: the number of frames to be used
--input_mean, -b: input mean
--input_std, -s: input standard deviation
--profiling, -p: [0|1], profiling or not
--threads, -t: number of threads

* Segmentation: run "tflite_segmentation" with command usage as below
--tflite_model, -m: model_name.tflite
--input_src, -r: [0|1|2] input source: image 0, video 1, camera 2
--input_path, -i: path of the input image/video or video port for camera, e.g., 1 for /dev/video1
--frame_cnt, -c: the number of frames to be used
--input_mean, -b: input mean
--input_std, -s: input standard deviation
--profiling, -p: [0|1], profiling or not
--threads, -t: number of threads

Demos packaged with Processor SDK
---------------------------------
* Classification demo with Arm only, full TIDL offload, and partial TIDL offload (heterogeneous execution)
  # cd /usr/share/tensorflow-lite/demos
  # ./run_tidl_compiler.sh  [This is creating mobilenet_v1_1.0_224_tidl_am5.tflite, .bin files, and subgraph0.cfg for full TIDL offload]
  # ./run_classification.sh [This runs Arm only classification first, and then with full TIDL offload]
  # ./run_tidl_compiler1.sh [This is creating mobilenet_v1_1.0_224_tidl_am5.tflite, .bin files, and subgraph0.cfg for partial TIDL offload,]
                            [specifying MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6 as the output tensor of the TIDL subgraph]
  # ./run_classification.sh [This runs Arm only classification first, and then with partial TIDL offload]

* Arm only segmentation demo:
  # cd /usr/share/tensorflow-lite/demos
  # ./run_segmentation.sh
